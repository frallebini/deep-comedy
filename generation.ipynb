{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generation.ipynb","provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb","timestamp":1620210143289}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"J0Qjg6vuaHNt"},"source":["# Generation"]},{"cell_type":"markdown","metadata":{"id":"swymtxpl7W7w"},"source":["## Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zvwNfOVkBzPP","executionInfo":{"status":"ok","timestamp":1627651757678,"user_tz":-120,"elapsed":33,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"773812d0-30c6-49fb-af02-5a697bcd62ca"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","%cd '/content/drive/My Drive/Deep Comedy/src'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/My Drive/Deep Comedy/src\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JjJJyJTZYebt"},"source":["import logging\n","import re\n","import time\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","from masking import create_masks\n","from transformer import Transformer\n","from learning_rate_scheduler import CustomSchedule\n","from syllabification import syllabify, get_tokenizers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXzVhU34zWEU"},"source":["logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0ZASlzCPCg-"},"source":["## Prepare the dataset"]},{"cell_type":"markdown","metadata":{"id":"iClMCAYDTi0-"},"source":["### Download and collect\n","\n","Download the syllabified Divine Comedy text from [[1]](#asperti)."]},{"cell_type":"code","metadata":{"id":"GZDcTQ0jFC0M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651760206,"user_tz":-120,"elapsed":2547,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"89a55f52-fab6-441b-cc85-ca1d5266e64b"},"source":["url = 'https://raw.githubusercontent.com/asperti/Dante/main'\n","\n","names = ['inferno_syllnew.txt', 'purgatorio_syllnew.txt', 'paradiso_syllnew.txt']\n","\n","paths = [tf.keras.utils.get_file(name, origin=f'{url}/{name}') for name in names]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://raw.githubusercontent.com/asperti/Dante/main/inferno_syllnew.txt\n","327680/319986 [==============================] - 0s 0us/step\n","Downloading data from https://raw.githubusercontent.com/asperti/Dante/main/purgatorio_syllnew.txt\n","327680/321559 [==============================] - 0s 0us/step\n","Downloading data from https://raw.githubusercontent.com/asperti/Dante/main/paradiso_syllnew.txt\n","319488/314474 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6u0m7m6e--fG"},"source":["def cleanup(verse):\n","    verse = re.sub(r'[0-9]+', '', verse)  # remove verse numeration\n","    verse = re.sub(r'[!\"(),-.:;?«»—‘“”]+', '', verse)  # remove \"special\" characters\n","    verse = verse.strip()\n","    return f'<v>{verse}|</v>'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_W9UOWQu8pvZ"},"source":["> `<v>` and `</v>` are the **_begin-of-verse_** and **_end-of-verse_** tokens, respectively."]},{"cell_type":"code","metadata":{"id":"ZNqiC_JQB0-s"},"source":["def collect_verses(path):\n","    with open(path) as f:\n","        # remove blank lines\n","        return [cleanup(line) for line in f if line != '\\n']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bC6KsAHEG4Em"},"source":["verses = [verse for path in paths for verse in collect_verses(path)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOpXJTyK3665"},"source":["verse_inside_canto_count = 0\n","\n","for i in range(len(verses)):\n","    if '•' in verses[i]:\n","        # current verse is a header:\n","        # put a </t> at the end of the previous verse,\n","        # which is the last verse of a canto,\n","        # and as such it will be considered as part\n","        # of a \"special\" single-verse tercet.\n","        if i != 0: \n","            verses[i-1] = f'{verses[i-1]}|</t>'\n","        verse_inside_canto_count = 0  # reset when at the beginning of a new canto\n","    else:\n","        if verse_inside_canto_count % 3 == 0:\n","            # first verse of a tercet\n","            verses[i] = f'<t>|{verses[i]}'\n","        if verse_inside_canto_count % 3 == 2:\n","            # last verse of a tercet\n","            verses[i] = f'{verses[i]}|</t>'\n","        verse_inside_canto_count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2Bt34_Y9Ndq"},"source":["> `<t>` and `</t>` are the **_begin-of-tercet_** and **_end-of-tercet_** tokens, respectively."]},{"cell_type":"code","metadata":{"id":"tTwPVrabi_SX"},"source":["# last verse of the whole Comedy has not been taken care of,\n","# since there is no header after it\n","verses[-1] = f'{verses[-1]}|</t>'\n","\n","# remove headers\n","verses = [verse for verse in verses if '•' not in verse]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w6cgo8QekRIX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651760215,"user_tz":-120,"elapsed":61,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"0934fdd7-69b5-4380-da67-17e1024f58fc"},"source":["verses[:6]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<t>|<v>|Nel |mez|zo |del |cam|min |di |no|stra |vi|ta|</v>',\n"," '<v>|mi |ri|tro|vai |per |u|na |sel|va o|scu|ra|</v>',\n"," '<v>|ché |la |di|rit|ta |via |e|ra |smar|ri|ta|</v>|</t>',\n"," '<t>|<v>|Ahi |quan|to a |dir |qual |e|ra è |co|sa |du|ra|</v>',\n"," '<v>|e|sta |sel|va |sel|vag|gia e |a|spra e |for|te|</v>',\n"," '<v>|che |nel |pen|sier |ri|no|va |la |pa|u|ra|</v>|</t>']"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"5pTHBn6d6H_f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651760217,"user_tz":-120,"elapsed":55,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"2cb23414-cdb8-4b14-b82d-ae30130b93bb"},"source":["verses[132:139]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<t>|<v>|che |tu |mi |me|ni |là |do|v’ or |di|ce|sti|</v>',\n"," '<v>|sì |ch’ io |veg|gia |la |por|ta |di |san |Pie|tro|</v>',\n"," '<v>|e |co|lor |cui |tu |fai |co|tan|to |me|sti|</v>|</t>',\n"," '<t>|<v>|Al|lor |si |mos|se e |io |li |ten|ni |die|tro|</v>|</t>',\n"," '<t>|<v>|Lo |gior|no |se |n’ an|da|va e |l’ ae|re |bru|no|</v>',\n"," '<v>|to|glie|va |li a|ni|mai |che |so|no in |ter|ra|</v>',\n"," '<v>|da |le |fa|ti|che |lo|ro e |io |sol |u|no|</v>|</t>']"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"7BXlgqezmnBM"},"source":["> _**Allor si mosse, e io li tenni dietro**_ \n","is the last verse of the first Canto of the Inferno, and, according to our notation, it constitutes a tercet all by itself."]},{"cell_type":"code","metadata":{"id":"XJgnv_Ucl4Lw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651760219,"user_tz":-120,"elapsed":46,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"c472d088-6331-4efb-814f-a0503ed62da0"},"source":["verses[274:281]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<t>|<v>|Or |va |ch’ un |sol |vo|le|re è |d’ am|be|due|</v>',\n"," '<v>|tu |du|ca |tu |se|gno|re e |tu |ma|e|stro|</v>',\n"," '<v>|Co|sì |li |dis|si e |poi |che |mos|so |fue|</v>|</t>',\n"," '<t>|<v>|in|trai |per |lo |cam|mi|no al|to e |sil|ve|stro|</v>|</t>',\n"," '<t>|<v>|Per |me |si |va |ne |la |cit|tà |do|len|te|</v>',\n"," '<v>|per |me |si |va |ne |l’ et|ter|no |do|lo|re|</v>',\n"," '<v>|per |me |si |va |tra |la |per|du|ta |gen|te|</v>|</t>']"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"IShE1s4tncuw"},"source":["> _**intrai per lo cammino alto e silvestro**_ \n","is the last verse of the second Canto of the Inferno, and again, according to our notation, it constitutes a tercet all by itself."]},{"cell_type":"code","metadata":{"id":"y7UsjI0Mkbx0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651760220,"user_tz":-120,"elapsed":40,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"8249bd67-b5d3-4f8c-8a5f-5d8eb75a27fb"},"source":["verses[-7:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<t>|<v>|ma |non |e|ran |da |ciò |le |pro|prie |pen|ne|</v>',\n"," '<v>|se |non |che |la |mia |men|te |fu |per|cos|sa|</v>',\n"," '<v>|da |un |ful|go|re in |che |sua |vo|glia |ven|ne|</v>|</t>',\n"," '<t>|<v>|A |l’ al|ta |fan|ta|sia |qui |man|cò |pos|sa|</v>',\n"," '<v>|ma |già |vol|ge|va il |mio |di|sio |e ’l |vel|le|</v>',\n"," '<v>|sì |co|me |ro|ta |ch’ i|gual|men|te è |mos|sa|</v>|</t>',\n"," '<t>|<v>|l’ a|mor |che |mo|ve il |so|le e |l’ al|tre |stel|le|</v>|</t>']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"2Jk6QzV1nvZH"},"source":["> The same holds for _**l’amor che move il sole e l’altre stelle**_, which is the last verse of the last Canto of the Paradiso, and, as such, the last verse of the whole Comedy."]},{"cell_type":"code","metadata":{"id":"ycfCsujuLz9S"},"source":["NUM_VERSES = len(verses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6fEfRRxQ9jv7"},"source":["The dataset will be a collection of `(input,target)` couples, where:\n","\n","* `input` are three verses from the Divine Comedy.\n","* `target` is the verse that follows the `input` verses."]},{"cell_type":"code","metadata":{"id":"wQSZGiMfitA6"},"source":["input = []\n","target = []\n","for i in range(NUM_VERSES-3):\n","    input.append(verses[i] + '|' + verses[i+1] + '|' + verses[i+2])\n","    target.append(verses[i+3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90hM39m6y2Si","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1627651760224,"user_tz":-120,"elapsed":33,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"1659362d-54f2-40bd-eb5f-62c28f19fa5f"},"source":["input[-1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<t>|<v>|A |l’ al|ta |fan|ta|sia |qui |man|cò |pos|sa|</v>|<v>|ma |già |vol|ge|va il |mio |di|sio |e ’l |vel|le|</v>|<v>|sì |co|me |ro|ta |ch’ i|gual|men|te è |mos|sa|</v>|</t>'"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"ULrXSt6Zy-WJ","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1627651760626,"user_tz":-120,"elapsed":13,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"41edf0dc-7710-475a-f8f3-ab5b18312bc8"},"source":["target[-1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<t>|<v>|l’ a|mor |che |mo|ve il |so|le e |l’ al|tre |stel|le|</v>|</t>'"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"jXWMzikXh3qW"},"source":["FIRST_TERCET = input[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sSAEleN9VDT6"},"source":["### Tokenize\n","\n","The dataset will be tokenized by syllables."]},{"cell_type":"code","metadata":{"id":"X2PCcu2ywh9z"},"source":["tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', split='|')\n","tokenizer.fit_on_texts(verses)\n","\n","tensor_input = tokenizer.texts_to_sequences(input)\n","tensor_input = tf.keras.preprocessing.sequence.pad_sequences(tensor_input, padding='post').astype(np.int64)\n","\n","tensor_target = tokenizer.texts_to_sequences(target)\n","tensor_target = tf.keras.preprocessing.sequence.pad_sequences(tensor_target, padding='post').astype(np.int64)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zxr3eHNpSkgJ"},"source":["> Notice that the `Tokenizer` instance has the `lower` argument set to `True` (by default), therefore the text will be converted to lowercase."]},{"cell_type":"code","metadata":{"id":"NghBXOPx0R8F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651761774,"user_tz":-120,"elapsed":55,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"2091deea-81ed-494a-b729-59dd522ba0d7"},"source":["tensor_input[-1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   3,    1,   70,   92,   48,  307,   18,  348,  182,  181, 1495,\n","        318,   40,    2,    1,   36,  136,   86,  139,  810,  125,    9,\n","        691,  361,  346,   54,    2,    1,   22,    7,   28,   58,   48,\n","       1873, 1109,   71, 1558,  325,   40,    2,    4,    0,    0,    0])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"HLMoJLCT4A5u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651761776,"user_tz":-120,"elapsed":45,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"9c235dfe-4c7c-43cf-a486-972844bde645"},"source":["tokenizer.sequences_to_texts([tensor_input[-1]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<t> <v> a  l’ al ta  fan ta sia  qui  man cò  pos sa </v> <v> ma  già  vol ge va il  mio  di sio  e ’l  vel le </v> <v> sì  co me  ro ta  ch’ i gual men te è  mos sa </v> </t>']"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"l0y-B4wAuqrQ"},"source":["Let's make the output of `sequences_to_texts` a bit more readable:"]},{"cell_type":"code","metadata":{"id":"brjoLYcYwh3a"},"source":["def sequences_to_texts(sequences):\n","    texts = []\n","    for sequence in sequences:\n","        text = ''\n","        for i in sequence:\n","            if i != 0 and i != tokenizer.word_index['<t>'] and i != tokenizer.word_index['<v>']:\n","                if i == tokenizer.word_index['</t>']:\n","                    text += '\\n'\n","                elif i == tokenizer.word_index['</v>']:\n","                    text = text[:-1]\n","                    text += '\\n'\n","                else:\n","                    text += tokenizer.index_word[i] + '|'\n","        texts.append(text)\n","    return texts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KtvFaESwu4-R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651761778,"user_tz":-120,"elapsed":35,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"8307fad9-fc21-47c7-b35a-5bf7f25f8ebf"},"source":["print(sequences_to_texts([tensor_input[-1]])[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["a |l’ al|ta |fan|ta|sia |qui |man|cò |pos|sa\n","ma |già |vol|ge|va il |mio |di|sio |e ’l |vel|le\n","sì |co|me |ro|ta |ch’ i|gual|men|te è |mos|sa\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0P74i2cu60g7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651761779,"user_tz":-120,"elapsed":25,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"14494ee3-6d9e-4064-b6f2-1541324259f8"},"source":["tensor_target[-1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   3,    1,  148,  308,    5,   50, 1233,   21,  526,   92,  171,\n","        383,   54,    2,    4])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"LGc_mCx508Sq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651762324,"user_tz":-120,"elapsed":560,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"9be02613-f623-411a-fcd8-ff1690d46157"},"source":["print(sequences_to_texts([tensor_target[-1]])[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["l’ a|mor |che |mo|ve il |so|le e |l’ al|tre |stel|le\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cMtaKYVvGJ2i"},"source":["VOCAB_SIZE = len(tokenizer.word_index)  # number of tokens (= syllables) in the vocaboulary\n","INPUT_SEQ_SIZE = len(tensor_input[0])  # length of a sequence encoding an input data (= tercet)\n","TARGET_SEQ_SIZE = len(tensor_target[0])  # length of a sequence encoding a target data (= tercet + next verse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tl_HjV0HaOf7"},"source":["### Create the dataset"]},{"cell_type":"code","metadata":{"id":"vamV3NHjABKR"},"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = len(input)\n","\n","dataset = tf.data.Dataset.from_tensor_slices((tensor_input, tensor_target)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZPvfWowbc1B4"},"source":["## Define the model"]},{"cell_type":"markdown","metadata":{"id":"zVjWCxFNcgbt"},"source":["### Set the hyperparameters\n"]},{"cell_type":"code","metadata":{"id":"lnJn5SLA2ahP"},"source":["NUM_LAYERS = 2\n","D_MODEL = 128\n","DFF = 256\n","NUM_HEADS = 2\n","DROPOUT_RATE = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jw9Ml-dME7uv"},"source":["> The values used in the base model of the original transformer [[2]](#attention) are \n","```\n","NUM_LAYERS=6\n","D_MODEL=512\n","DFF=2048\n","NUM_HEADS=8\n","DROPOUT_RATE=0.1\n","```"]},{"cell_type":"code","metadata":{"id":"UiysUa--4tOU"},"source":["generator = Transformer(\n","    num_layers=NUM_LAYERS,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dff=DFF,\n","    input_vocab_size=VOCAB_SIZE+1,\n","    target_vocab_size=VOCAB_SIZE+1,\n","    pe_input=1000,\n","    pe_target=1000,\n","    rate=DROPOUT_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GOmWW--yP3zx"},"source":["### Choose the optimizer\n"]},{"cell_type":"code","metadata":{"id":"7r4scdulztRx"},"source":["optimizer = tf.keras.optimizers.Adam(CustomSchedule(D_MODEL), \n","                                     beta_1=0.9, beta_2=0.98, epsilon=1e-9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oxGJtoDuYIHL"},"source":["### Choose the metrics\n"]},{"cell_type":"code","metadata":{"id":"MlhsJMm0TW_B"},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zzkhr2MAFaim"},"source":["> Since the target sequences are padded, it is important to apply a padding mask when calculating loss and accuracy."]},{"cell_type":"code","metadata":{"id":"67oqVHiT0Eiu"},"source":["def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","\n","\n","def accuracy_function(real, pred):\n","  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n","\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  accuracies = tf.math.logical_and(mask, accuracies)\n","\n","  accuracies = tf.cast(accuracies, dtype=tf.float32)\n","  mask = tf.cast(mask, dtype=tf.float32)\n","  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phlyxMnm-Tpx"},"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aeHumfr7zmMa"},"source":["## Train"]},{"cell_type":"markdown","metadata":{"id":"0Di_Yaa1gf9r"},"source":["In the following, the target is divided into `tar_inp` and `tar_real`: \n","* `tar_inp` is passed as an input to the decoder.\n","* `tar_real` is that same input shifted by 1: at each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n","\n","The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n","\n","During training we use **_teacher-forcing_**, i.e. passing the true output to the next time step regardless of what the model predicts at the current time step.\n","\n","As the transformer predicts each token, self-attention allows it to look at the previous tokens in the input sequence to better predict the next token.\n","\n","To prevent the model from peeking at the expected output, the model uses a look-ahead mask."]},{"cell_type":"code","metadata":{"id":"LKpoA6q1sJFj"},"source":["EPOCHS = 80"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5bYfloQTl8w"},"source":["train_step_signature = [\n","    tf.TensorSpec(shape=(BATCH_SIZE, INPUT_SEQ_SIZE), dtype=tf.int64),\n","    tf.TensorSpec(shape=(BATCH_SIZE, TARGET_SEQ_SIZE), dtype=tf.int64)\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iJwmp9OE29oj"},"source":["@tf.function(input_signature=train_step_signature)\n","def train_step(inp, tar):\n","  tar_inp = tar[:, :-1]\n","  tar_real = tar[:, 1:]\n","\n","  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","  with tf.GradientTape() as tape:\n","    predictions, _ = generator(inp, tar_inp,\n","                                 True,\n","                                 enc_padding_mask,\n","                                 combined_mask,\n","                                 dec_padding_mask)\n","    loss = loss_function(tar_real, predictions)\n","\n","  gradients = tape.gradient(loss, generator.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n","\n","  train_loss(loss)\n","  train_accuracy(accuracy_function(tar_real, predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZTyyegFDam_N","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1627651768799,"user_tz":-120,"elapsed":43,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"96798595-9137-434d-aac2-d7a5213836bb"},"source":["'''for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","\n","  for (batch, (inp, tar)) in enumerate(dataset):\n","    train_step(inp, tar)\n","\n","    if batch % 50 == 0:\n","      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n","\n","  print(f'\\n\\t---Results Epoch {epoch + 1}---')\n","  print(f'Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n","    \n","  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"for epoch in range(EPOCHS):\\n  start = time.time()\\n\\n  train_loss.reset_states()\\n  train_accuracy.reset_states()\\n\\n  for (batch, (inp, tar)) in enumerate(dataset):\\n    train_step(inp, tar)\\n\\n    if batch % 50 == 0:\\n      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\\n\\n  print(f'\\n\\t---Results Epoch {epoch + 1}---')\\n  print(f'Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\\n    \\n  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\""]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"YeBYLx2GXXrz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651768801,"user_tz":-120,"elapsed":41,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"dd6b097e-f2fd-48a2-95d7-4df9d38da7d4"},"source":["%cd '/content/drive/My Drive/Deep Comedy'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Deep Comedy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nu9uXJ8wlV8B"},"source":["#generator.save_weights('generation_weights/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIHGs_slCOhn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651770883,"user_tz":-120,"elapsed":2109,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"be80e3c7-8320-44b4-eb11-64998953d2e8"},"source":["generator.load_weights('generation_weights/')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2183b40550>"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"F_jNgxvFeoEj"},"source":["## Generate\n","\n","The generation process unfolds through the following steps:\n","\n","1. The first tercet of the Divine Comedy is given as input to the encoder.\n","2. The decoder input is initialized to the start-of-tercet (`<t>`) token.\n","3. Calculate the padding masks and the look ahead masks.\n","4. The model makes predictions of the next token for each token in the output. Most of these are redundant: use the predictions from the last token.\n","5. Concatenate the predicted token to the decoder input and pass it to the decoder itself.\n","6. Once a whole verse has been generated, remove the `|` symbols from the verse and give it to the syllabifier: \n","    * If the syllabifier output matches the verse as it was generated, count the syllables:\n","        * If there are 11 of them, go to step 7.\n","        * Otherwise, reset the decoder input to its first token (either `<t>` or `<v>`) and go back to step 3.\n","    * Otherwise, reset the decoder input to its first token (either `<t>` or `<v>`) and go back to step 3.\n","7. Remove the first verse from the encoder input.\n","8. Append the newly predicted verse to the encoder input.\n","9. Reset the decoder input to to its first token (either `<t>` or `<v>`) and go back to step 3."]},{"cell_type":"markdown","metadata":{"id":"8U5P-2s56bHp"},"source":["### Load the syllabifier"]},{"cell_type":"code","metadata":{"id":"tHWVglYJpQNT"},"source":["syllabifier = Transformer(\n","    num_layers=2,\n","    d_model=128,\n","    num_heads=2,\n","    dff=256,\n","    input_vocab_size=80+1,\n","    target_vocab_size=81+1,\n","    pe_input=1000,\n","    pe_target=1000,\n","    rate=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZuBZeRoreT8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651773275,"user_tz":-120,"elapsed":1416,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"c80deeb2-b927-4550-aff0-e688f39025fb"},"source":["syllabifier.load_weights('syllabification_weights/')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f21837c7d10>"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"nlYU-V62qDlJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651775510,"user_tz":-120,"elapsed":2252,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"d33cbf23-270d-4d5f-bafc-ca94e62dca41"},"source":["tokenizer_nosyll, tokenizer_syll = get_tokenizers()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://raw.githubusercontent.com/asperti/Dante/main/inferno.txt\n","204800/203103 [==============================] - 0s 0us/step\n","Downloading data from https://raw.githubusercontent.com/asperti/Dante/main/purgatorio.txt\n","204800/202955 [==============================] - 0s 0us/step\n","Downloading data from https://raw.githubusercontent.com/asperti/Dante/main/paradiso.txt\n","204800/199196 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8928q4ZL6pDK"},"source":["### Define the generation process"]},{"cell_type":"code","metadata":{"id":"OX3Wdko7BqIw"},"source":["def predict(encoder_input, decoder_input, k=10):\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, decoder_input)\n","\n","    # predictions.shape == (batch_size, seq_len, vocab_size)\n","    predictions, attention_weights = generator(encoder_input,\n","                                               decoder_input,\n","                                               False,\n","                                               enc_padding_mask,\n","                                               combined_mask,\n","                                               dec_padding_mask)\n","\n","    # select the last character from the seq_len dimension\n","    predictions = predictions[:, -1:, :]  # (1, 1, vocab_size)\n","    \n","    # top-k sampling\n","    predictions = tf.squeeze(predictions)  # (,vocab_size)\n","    logits, indices = tf.math.top_k(predictions, k)\n","    probs = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n","\n","    indices = np.asarray(indices).astype('int32')\n","    probs = np.asarray(probs).astype('float32')\n","    predicted_id = np.random.choice(indices, p=probs)\n","\n","    return tf.constant(predicted_id, dtype=tf.int64, shape=(1,1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JFQW03za7rWq"},"source":["> The next token is predicted by adopting **_top-k sampling_** as sampling method (with `k=10` as default)."]},{"cell_type":"code","metadata":{"id":"22Oqd62-Q_z1"},"source":["def has_issues(decoder_input):\n","    verse = sequences_to_texts(decoder_input.numpy())[0].replace('\\n','')\n","    input_syll = f'<{verse.replace(\"|\",\"\")}>'\n","    \n","    output_syll = syllabify(\n","        input_syll, \n","        syllabifier, \n","        tokenizer_nosyll, \n","        tokenizer_syll)[0].replace('<|','').replace('>','')\n","\n","    if verse != output_syll:\n","        print('\\nFOUND CONFLICT:')\n","        print(f'{\"Generator output:\":20s} {verse}')\n","        print(f'{\"Syllabifier output:\":20s} {output_syll}')\n","        return True\n","\n","    num_syll = len(verse.split('|'))\n","    if num_syll != 11:\n","        print('\\nFOUND WRONG NUMBER OF SYLLABLES:')\n","        print(f'Generator output = Syllabifier output: {verse}')\n","        print(f'Number of syllables: {num_syll}')\n","        return True\n","    \n","    return False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpA90IBYvOz0"},"source":["def generate(k=10):\n","    encoder_input = tokenizer.texts_to_sequences([FIRST_TERCET])\n","    encoder_input = tf.convert_to_tensor(encoder_input, dtype=tf.int64)\n","    \n","    SOT = tf.constant(tokenizer.word_index['<t>'], dtype=tf.int64)\n","    EOT = tf.constant(tokenizer.word_index['</t>'], dtype=tf.int64)\n","    SOV = tf.constant(tokenizer.word_index['<v>'], dtype=tf.int64)\n","    EOV = tf.constant(tokenizer.word_index['</v>'], dtype=tf.int64)\n","    \n","    decoder_input = tf.convert_to_tensor([SOT], dtype=tf.int64)\n","    decoder_input = tf.expand_dims(decoder_input, axis=0)\n","    initial_decoder_input = decoder_input\n","    final_decoder_input = decoder_input\n","    \n","    verse_count = 0\n","    strings = []\n","    \n","    while verse_count < 33:\n","        # concatenate the predicted_id to the output,\n","        # which is then given to the decoder as its own input\n","        predicted_id = predict(encoder_input, decoder_input, k)\n","        decoder_input = tf.concat([decoder_input, predicted_id], axis=-1)\n","        \n","        if predicted_id == EOV:\n","\n","            if has_issues(decoder_input):\n","                # the syllabification produced by the syllabifier differs from that of the generator,\n","                # therefore discard the verse and generate a new one\n","                decoder_input = initial_decoder_input\n","                continue\n","\n","            if (verse_count + 1) % 3 == 0:\n","                # 3 verses have been generated, i.e. we are at the end of a tercet\n","                eot = tf.convert_to_tensor([EOT], dtype=tf.int64)\n","                eot = tf.expand_dims(eot, axis=0)\n","                decoder_input = tf.concat([decoder_input, eot], axis=-1)\n","\n","            # remove first verse from encoder input\n","            encoder_input = encoder_input.numpy()\n","            i = np.where(encoder_input[0] == EOV.numpy())[0][0]\n","            encoder_input = encoder_input[:, i+1:]\n","            encoder_input = tf.convert_to_tensor(encoder_input, dtype=tf.int64)\n","            \n","            # concatenate newly generated verse to encoder input\n","            encoder_input = tf.concat([encoder_input, decoder_input], axis=-1)\n","\n","            # save generated verse for final display\n","            final_decoder_input = tf.concat([final_decoder_input, decoder_input], axis=-1)\n","\n","            # restart decoder input from either SOV or SOT token\n","            if decoder_input[0,-1] == EOT.numpy(): \n","                decoder_input = tf.convert_to_tensor([SOT], dtype=tf.int64)\n","            else:\n","                decoder_input = tf.convert_to_tensor([SOV], dtype=tf.int64)\n","            decoder_input = tf.expand_dims(decoder_input, axis=0)\n","            initial_decoder_input = decoder_input\n","            \n","            verse_count += 1\n","        \n","    return sequences_to_texts(final_decoder_input.numpy())[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LaU88cFm6wgP"},"source":["### Display the results"]},{"cell_type":"code","metadata":{"id":"NTUAA8tUzsdn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651955174,"user_tz":-120,"elapsed":179670,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"7ff20e1a-423d-44d2-ac6a-5ae712f9eab5"},"source":["generated_verses = generate()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","FOUND WRONG NUMBER OF SYLLABLES:\n","Generator output = Syllabifier output: pa|re|va |già |nel |cam|bian|do |par|ran |trat|te\n","Number of syllables: 12\n","\n","FOUND WRONG NUMBER OF SYLLABLES:\n","Generator output = Syllabifier output: col |ca|ri|me e|ra|no |la|vi ’l |boc|ca\n","Number of syllables: 10\n","\n","FOUND WRONG NUMBER OF SYLLABLES:\n","Generator output = Syllabifier output: fu|ron |cre|a|te a |bel|la |don|na |via\n","Number of syllables: 10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mS-T2yuYoLN9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651955175,"user_tz":-120,"elapsed":78,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"a7793e29-2315-4111-b066-c52e2df777d0"},"source":["print(generated_verses)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ahi |quan|to a |dir |qual |e|ra è |co|sa |du|ra\n","e|sta |sel|va |sel|vag|gia e |a|spra e |for|te\n","che |nel |pen|sier |ri|no|va |la |pa|u|ra\n","\n","tan|t’ è |l’ or|ri|mi|ra |di |sé |ri|mor|te\n","per |che |del |po|sto |mor|tal |quan|do |pon|ti\n","che |per |lo |de|mo|lar |ne’ |par|ve|re o|te\n","\n","o|gne |vir|tù |for|vien |che |s’ io |ri|dir|ti\n","dis|s’ io |a |me |tal|vol|ta |sì |com’ |a|mo\n","se |non |eb|be |po|suo|ta |li |sì |for|ti\n","\n","vi|di |per |lo |no|stro a|mor |dis|se |co|mo\n","io |nol |di|scon|dea |sì |ch’ al|tra |fï|a|te\n","lo |ciel |che |pian|ger |più |sta|van |di |fo|mo\n","\n","lo |mio |ma|e|stro |son |sì |su|so il |sguar|te\n","in |cac|cia|ti e |che ’l |so|lo è |sì |par|te|re\n","co|min|ciò |tut|ti |li al|tri |che ’l |ciel |fron|te\n","\n","lo |du|ca |mio |si |vol|te in |ti|ca |fi|re\n","lo |fie|re |de |la |mor|te |dis|se |scoc|ca\n","l’ ar|ti|co |del |cam|mem|bru|to e |che |fo|re\n","\n","e |nes|sun |ri|pa|re in|te|bru|na |bar|ca\n","li |ri|chiu|se il |fum|mo e |quin|di |ri|pa |vò\n","do|ve |can|ta|re |do|ve |la |com|par|ca\n","\n","co|me |la |vo|ce |vo|lon|tier |co|no|vo\n","ma |cia|scu|na |bel|la |don|na |cin|bi|le\n","ti |fac|cia |tut|to ’l |mio |che |ti |di|ste|vo\n","\n","e |quel|la |don|na |che |mi |sca|la in|se|le\n","di|nan|zi |che |l’ al|tra |roc|cia |son |quel|la\n","ne |le |gam|be |con |le |mar|che|ru|scel|le\n","\n","ma |pri|ma e|qua|li|tà |v’ ap|par|se |bel|la\n","d’ un |pe|so |per |cia|scun |di |quei |ch’ a|scol|ta\n","in |u|na |sï|on |ch’ a|vea |la |fa|gil|la\n","\n","lo |lu|vio |quan|do |vi|di |que|ste |tol|ta\n","ma |vi|di |qui |ne |la |fiam|ma |cre|a|ta\n","e |la |vir|tu|za |vir|tù |che |già |vol|ta\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lnAoFu19MH4b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627651955176,"user_tz":-120,"elapsed":70,"user":{"displayName":"Fabio Galvan","photoUrl":"","userId":"05181207407050492677"}},"outputId":"e49c5cd0-440b-4cf3-811a-1730eff6fd51"},"source":["print(generated_verses.replace('|',''))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ahi quanto a dir qual era è cosa dura\n","esta selva selvaggia e aspra e forte\n","che nel pensier rinova la paura\n","\n","tant’ è l’ orrimira di sé rimorte\n","per che del posto mortal quando ponti\n","che per lo demolar ne’ parvere ote\n","\n","ogne virtù forvien che s’ io ridirti\n","diss’ io a me talvolta sì com’ amo\n","se non ebbe posuota li sì forti\n","\n","vidi per lo nostro amor disse como\n","io nol discondea sì ch’ altra fïate\n","lo ciel che pianger più stavan di fomo\n","\n","lo mio maestro son sì suso il sguarte\n","in cacciati e che ’l solo è sì partere\n","cominciò tutti li altri che ’l ciel fronte\n","\n","lo duca mio si volte in tica fire\n","lo fiere de la morte disse scocca\n","l’ artico del cammembruto e che fore\n","\n","e nessun ripare intebruna barca\n","li richiuse il fummo e quindi ripa vò\n","dove cantare dove la comparca\n","\n","come la voce volontier conovo\n","ma ciascuna bella donna cinbile\n","ti faccia tutto ’l mio che ti distevo\n","\n","e quella donna che mi scala insele\n","dinanzi che l’ altra roccia son quella\n","ne le gambe con le marcheruscelle\n","\n","ma prima equalità v’ apparse bella\n","d’ un peso per ciascun di quei ch’ ascolta\n","in una sïon ch’ avea la fagilla\n","\n","lo luvio quando vidi queste tolta\n","ma vidi qui ne la fiamma creata\n","e la virtuza virtù che già volta\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RqQ1fIsLwkGE"},"source":["## References\n","<a name=\"asperti\">[1]</a> [`Dante` repository at prof. Asperti's GitHub page](https://github.com/asperti/Dante)\n","<br>\n","<a name=\"attention\">[2]</a> [Attention Is All You Need, Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)"]}]}